{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "proj4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiK4QNWKDvWJ"
      },
      "source": [
        "# [Scene Recognition with Deep Learning](https://www.cc.gatech.edu/~hays/compvision/proj4/)\n",
        "For this project we are going to focus on scene classification for 15 scene types with a state-of-the-art approach: deep learning. The task is also known as image classification. \n",
        "\n",
        "Basic learning objectives of this project:\n",
        "1. Construct the fundamental pipeline for performing deep learning using PyTorch;\n",
        "2. Understand the concepts behind different layers, optimizers, and learning schedules;\n",
        "3. Experiment with different models and observe the performance.\n",
        "\n",
        "The starter code is mostly initialized to 'placeholder' just so that the starter\n",
        "code does not crash when run unmodified and you can get a preview of how\n",
        "results are presented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbtuSRZAhb9a"
      },
      "source": [
        "# flag to modify everything to run better on Colab; change it to true if you want to run on colab\n",
        "use_colab = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uto3xotovm5_"
      },
      "source": [
        "## Part 0: Setup for Colab\n",
        "You can skip this part if you are not running your notebook on Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMShUvcNvukQ"
      },
      "source": [
        "### Download Data\n",
        "\n",
        "Download the data for training the network. It's exactly the same as that's been provided for you, but we'll fetch this from the cloud to keep uploads small"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG_t11Chv9pq"
      },
      "source": [
        "# uncomment for running on colab\n",
        "# !wget \"https://cc.gatech.edu/~hays/compvision/projects/proj4_data.zip\" -O data.zip && unzip -qq data.zip\n",
        "# !rm ./data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ9Qm5hnxKNn"
      },
      "source": [
        "### Upload code and unit tests\n",
        "\n",
        "Once you have finished your code, run `python ./zip_for_colab.py` and all the required files and tests will be written to `cv_proj4.zip`.\n",
        "\n",
        "Click the folder icon on the left of the colab UI, and click on the upload button right below the \"Files\" heading. You should have done a similar process for Project 4.\n",
        "\n",
        "Run the cell below once your upload completes to extract your uploaded files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpoinJA1yCQy"
      },
      "source": [
        "# uncomment for running on colab\n",
        "# !unzip -qq cv_proj4.zip -d ./\n",
        "# !mv -v ./src/vision ./\n",
        "# !pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6O_juWwyI3X"
      },
      "source": [
        "### Preparation\n",
        "\n",
        "We'll import the required functions and set up GPU computation.\n",
        "\n",
        "Click on Runtime $\\rightarrow$ Change Runtime Type, and select \"GPU\" under hardware accelerator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1dqr6qSBpE2"
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "\n",
        "from vision.runner import Trainer\n",
        "from vision.optimizer import get_optimizer\n",
        "from vision.simple_net import SimpleNet\n",
        "from vision.simple_net_final import SimpleNetFinal\n",
        "from vision.my_resnet import MyResNet18\n",
        "from vision.data_transforms import (\n",
        "    get_fundamental_transforms,\n",
        "    get_fundamental_normalization_transforms,\n",
        "    get_fundamental_augmentation_transforms,\n",
        "    get_all_transforms,\n",
        ")\n",
        "from vision.stats_helper import compute_mean_and_std\n",
        "from vision.confusion_matrix import (\n",
        "    generate_confusion_data,\n",
        "    generate_confusion_matrix,\n",
        "    plot_confusion_matrix,\n",
        "    get_pred_images_for_target,\n",
        "    generate_and_plot_confusion_matrix,\n",
        ")\n",
        "from vision.dl_utils import save_trained_model_weights\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKX1cUvTvgjL"
      },
      "source": [
        "from tests.utils import verify\n",
        "from tests.test_stats_helper import test_mean_and_variance\n",
        "from tests.test_image_loader import (\n",
        "    test_dataset_length,\n",
        "    test_unique_vals,\n",
        "    test_class_values,\n",
        "    test_load_img_from_path,\n",
        ")\n",
        "from tests.test_data_transforms import (\n",
        "    test_fundamental_transforms,\n",
        "    test_data_augmentation_transforms,\n",
        "    test_data_augmentation_with_normalization_transforms,\n",
        ")\n",
        "from tests.test_dl_utils import test_compute_accuracy, test_compute_loss\n",
        "from tests.test_simple_net import test_simple_net\n",
        "from tests.test_simple_net_final import test_simple_net_final\n",
        "from tests.test_my_resnet import test_my_resnet\n",
        "from tests.test_confusion_matrix import (\n",
        "    test_generate_confusion_matrix,\n",
        "    test_generate_confusion_matrix_normalized,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjE0jIc5BpFN"
      },
      "source": [
        "is_cuda = True\n",
        "is_cuda = (\n",
        "    is_cuda and torch.cuda.is_available()\n",
        ")  # will turn off cuda if the machine doesnt have a GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSUp5MMshb9m"
      },
      "source": [
        "data_path = \"./data/\" if not use_colab else \"./data/\"\n",
        "model_path = \"../model_checkpoints/\" if not use_colab else \"./model_checkpoints/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGSv2QfBBpFZ"
      },
      "source": [
        "## Part 1: SimpleNet\n",
        "To train a network in PyTorch, we need 4 components:\n",
        "1. **Dataset** - an object which can load the data and labels given an index.\n",
        "2. **Model** - an object that contains the network architecture definition.\n",
        "3. **Loss function** - a function that measures how far the network output is from the ground truth label.\n",
        "4. **Optimizer** - an object that optimizes the network parameters to reduce the loss value.\n",
        "\n",
        "### Part 1.1: Datasets\n",
        "Now let's create the **Datasets** object to be used later. Remember back in Project 1, we have initialized such a class to load 5 images? Here the task is similar: we have to load each image as well as it's classification label. The key idea is to store the paths to all the images in your dataset, and then be able to provide the image file path and its ground truth class id when given the index of a data example.\n",
        "\n",
        "We will map the scene names (text) into indices 0 to 14 in the image loader. You can choose any mapping you want but once fixed, it has to be consistent throughout this notebook.\n",
        "\n",
        "**TODO 1:** complete the `image_loader.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THRvAvluXFcS"
      },
      "source": [
        "inp_size = (64, 64)\n",
        "print(\"Testing your image loader (length):\", verify(test_dataset_length))\n",
        "print(\"Testing your image loader (values):\", verify(test_unique_vals))\n",
        "print(\"Testing your image loader (classes):\", verify(test_class_values))\n",
        "print(\"Testing your image loader (paths):\", verify(test_load_img_from_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3TJtEB2vgjQ"
      },
      "source": [
        "### Data transforms\n",
        "**TODO 2:** complete the function `get_fundamental_transforms()` in `data_transforms.py` to compile the following fundamental transforms:\n",
        "1. Resize the input image to the desired shape;\n",
        "2. Convert it to a tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqpCmJoxvgjQ"
      },
      "source": [
        "print(\"Testing your fundamental data transforms: \", verify(test_fundamental_transforms))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnvQDyMfvgjR"
      },
      "source": [
        "### Part 1.2: Model\n",
        "The data is ready! Now we are preparing to move to the actual core of deep learning: the architecture. To get you started in this part, simply define a **2-layer** model in the `simple_net.py`. Here by \"2 layers\" we mean **2 convolutional layers**, so you need to figure out the supporting utilities like ReLU, Max Pooling, and Fully Connected layers, and configure them with proper parameters to make the tensor flow.\n",
        "\n",
        "You may refer to Figure 2 in proj4 handout for a sample network architecture (it's the architecture TAs used in their implementation and is sufficient to get you pass Part 1).\n",
        "\n",
        "**TODO 3**: Do the following in ```simple_net.py```:\n",
        "- ```self.conv_layers```\n",
        "- ```self.fc_layers```\n",
        "- ```forward()```\n",
        "\n",
        "Leave the ```self.loss_criterion = None``` for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvVL-ap0BpFx"
      },
      "source": [
        "print(\"Testing your SimpleNet architecture: \", verify(test_simple_net))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OTCOQNFvgjS"
      },
      "source": [
        "simple_model = SimpleNet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkbHMcEYvgjS"
      },
      "source": [
        "### Loss function\n",
        "When defining your model architecture, also initialize the `loss_criterion` variable there. Remeber this is multi-class classification problem, and choose the [appropriate loss function](https://pytorch.org/docs/stable/nn.html#loss-functions) might be useful here.\n",
        "\n",
        "**TODO 4:** Assign a loss function to ```self.loss_criterion``` in ```simple_net.py```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeBFNW0mvgjS"
      },
      "source": [
        "print(simple_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwEAGDYrvgjU"
      },
      "source": [
        "### Optimizer\n",
        "**TODO 5:** **initialize the following cell with proper values for learning rate and weight decay** (you will need to come back and tune these values for better performance once the trainer section is done)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2cwtK5PBpF7"
      },
      "source": [
        "# TODO: add a decent initial setting and tune from there. The values are intentionally bad.\n",
        "optimizer_config = {\"optimizer_type\": \"adam\", \"lr\": 1e-3, \"weight_decay\": 1e-1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkC1MXKZhb9w"
      },
      "source": [
        "**TODO 6:** complete the ```get_optimizer()``` function in ```optimizer.py```. The helper function accepts three basic configurations as defined below. Any other configuration is optional. *SGD* optimizer type should be supported, anything else is optional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0CrYZa4BpGE"
      },
      "source": [
        "optimizer = get_optimizer(simple_model, optimizer_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FffW3ZPhvgjU"
      },
      "source": [
        "### Part 1.3: Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBu49n5ghb9x"
      },
      "source": [
        "**TODO 7:** Next we define the trainer for the model; to start, we will need to do the following in ```dl_utils.py```:\n",
        "- ```compute_loss()```: use the model's loss criterion and compute the corresponding loss between the model output and the ground-truth labels.\n",
        "- ```compute_accuracy()```: compute the classification accuracy given the prediction logits and the ground-truth labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7vcKhQohb9y"
      },
      "source": [
        "print(\"Testing your trainer (loss values): \", verify(test_compute_loss))\n",
        "print(\"Testing your trainer (accuracy computation): \", verify(test_compute_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk060N9wvgjU"
      },
      "source": [
        "Then pass in the model, optimizer, transforms for both the training and testing datasets into the trainer, and proceed to the next cell to train it. If you have implemented everything correctly, you should be seeing a decreasing loss value.\n",
        "\n",
        "**Note** in this project, we will be using the test set as the validation set (i.e. using it to guide our decisions about models and hyperparameters while training. In actual practise, you would not interact with the test set until reporting the final results.\n",
        "\n",
        "**Note** that your CPU should be sufficient to handle the training process for all networks in this project, and the following training cells will take less than 5 minutes; you may also want to decrease the value for `num_epochs` and quickly experiment with your parameters. The default value of **30** is good enough to get you around the threshold for Part 1, and you are free to increase it a bit and adjust other parameters in this part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiGOvPJfBpGO"
      },
      "source": [
        "# re-init the model so that the weights are all random\n",
        "simple_model_base = SimpleNet()\n",
        "optimizer = get_optimizer(simple_model_base, optimizer_config)\n",
        "\n",
        "trainer = Trainer(\n",
        "    data_dir=data_path,\n",
        "    model=simple_model_base,\n",
        "    optimizer=optimizer,\n",
        "    model_dir=os.path.join(model_path, \"simple_net\"),\n",
        "    train_data_transforms=get_fundamental_transforms(inp_size),\n",
        "    val_data_transforms=get_fundamental_transforms(inp_size),\n",
        "    batch_size=32,\n",
        "    load_from_disk=False,\n",
        "    cuda=is_cuda,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paNLyU5cBpGX",
        "scrolled": false
      },
      "source": [
        "%%time\n",
        "trainer.run_training_loop(num_epochs=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDNaA9ekvgjV"
      },
      "source": [
        "After you have finished the training process, now plot out the loss and accuracy history. You can also check out the final accuracy for both training and testing data. Copy the accuracy plots and values onto the report, and answer the questions there. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0b_WwJhBpGf",
        "scrolled": false
      },
      "source": [
        "trainer.plot_loss_history()\n",
        "trainer.plot_accuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8epn0IBmBpGn",
        "scrolled": true
      },
      "source": [
        "train_accuracy = trainer.train_accuracy_history[-1]\n",
        "validation_accuracy = trainer.validation_accuracy_history[-1]\n",
        "print(\n",
        "    \"Train Accuracy = {}; Validation Accuracy = {}\".format(\n",
        "        train_accuracy, validation_accuracy\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfGo97oqhb92"
      },
      "source": [
        "**TODO 8:** Obtain **45%** validation accuracy to receive full credit for Part 1. You can go back to TODO 5 to tune your parameters using the following tips:\n",
        "1. If the loss decreases very slowly, try increasing the value of the lr (learning rate).\n",
        "2. Initially keep the value of weight decay (L2-regularization) very low.\n",
        "3. Try to first adjust lr in multiples of 3 initially. When you are close to reasonable performance, do a more granular adjustment.\n",
        "4. If you want to increase the validation accuracy by a little bit, try increasing the weight_decay to prevent overfitting. Do not use tricks from Section 6 just yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Ec7hcZhb92"
      },
      "source": [
        "### Save the model for your SimpleNet\n",
        "Once you are satisfied with the performance of your trained model, you need to save it so that you can upload it to Gradescope along with the other models.\n",
        "\n",
        "We'll save the model to the current directory. If you're running locally on your computer, this should be in the `vision` folder, which is the desired location for uploading to gradescope.\n",
        "\n",
        "If you are running on Colab, make sure you download the trained `.pt` files that will be generated. This process is similar to that of Project 4.\n",
        "- Click on the folder icon in the left hand side menu\n",
        "- Select the 3 dots next to the `<out_name>.pt` file that is generated and click download\n",
        "- Store the file in the `vision` folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUjbBOiDhb94"
      },
      "source": [
        "save_trained_model_weights(simple_model_base, out_dir=\"./\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9fcNntXhb94"
      },
      "source": [
        "## Part 2: SimpleNet with additional modifications\n",
        "\n",
        "In Part 1 we implemented a basic CNN, but it doesn’t perform very well.  Let’s try a few tricks to see if we canimprove our model performance. You can start by copying your `SimpleNet` architecture from `simple_net.py` into `SimpleNetFinal` class in `simple_net_final.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpI4Xda_hb-I"
      },
      "source": [
        "### Part 2.1: Problem 1 We don’t have enough training data. Let’s “jitter.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SqKl7l1hb-J"
      },
      "source": [
        "We are going to increase our amount of training data by left-right mirroring and color jittering the training images during the learning process.\n",
        "\n",
        "**TODO 9:** complete the `get_fundamental_augmentation_transforms()` function in `data_transforms.py`: first copy your existing fundamental transform implementation into this function, and then insert a couple of other transforms which help you do the above adjustment.                 \n",
        "\n",
        "Useful functions:`transforms.RandomHorizontalFlip`, `transforms.ColorJitter`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW9BDcJ0hb-J"
      },
      "source": [
        "print(\n",
        "    \"Testing your data transforms with data augmentation: \",\n",
        "    verify(test_data_augmentation_transforms),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eznl7cPhb-K"
      },
      "source": [
        "### Part 2.2: Problem 2 The images aren’t zero-centered and variance-normalized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ1Src4shb-K"
      },
      "source": [
        "We are going to \"zero-center\" and \"normalize\" the dataset so that each entry has zero mean and the overall standard deviation is 1. \n",
        "\n",
        "**TODO 10**:  fill in the `compute_mean_and_std()` in `stats_helper.py` to compute the **mean** and **standard deviation** of both training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjYWIY_Bhb-K"
      },
      "source": [
        "print(\"Testing your mean and std computation: \", verify(test_mean_and_variance))\n",
        "dataset_mean, dataset_std = compute_mean_and_std(data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FRdPZaWhb-L"
      },
      "source": [
        "print(\"Dataset mean = {}, standard deviation = {}\".format(dataset_mean, dataset_std))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ETCNQFzhb-L"
      },
      "source": [
        "**TODO 11**: complete the function `get_all_transforms()` function in `data_transforms.py` to normalize the input using the passed in mean andstandard deviation: you need to copy your implementation of `get_fundamental_augmentation_transforms()` into this function first.                 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VysOVwXfhb-L"
      },
      "source": [
        "print(\n",
        "    \"Testing your normalized data transforms: \",\n",
        "    verify(test_data_augmentation_with_normalization_transforms),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVLqAzmlhb-M"
      },
      "source": [
        "inp_size = (64, 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyUv4VRHhb-N"
      },
      "source": [
        "### Part 2.3-2.5: Problem 3 ~ 5: Modify the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_zfgLhrhb-N"
      },
      "source": [
        "**TODO 12:** modify the layers in the `SimpleNet` class in `simple_net.py`:\n",
        "1. Add the dropout layer\n",
        "2. Add one or two more blocks of “conv/pool/relu”.\n",
        "3. Add a batch normalization layer after each convolutional layer (except for the last)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cLHBt3Jhb-O"
      },
      "source": [
        "print(\"Testing your SimpleNetFinal architecture: \", verify(test_simple_net_final))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sRvxPmjhb-P"
      },
      "source": [
        "simple_model_final = SimpleNetFinal()\n",
        "print(simple_model_final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7tfVgjhhb-P"
      },
      "source": [
        "Similar to the previous part, **initialize the following cell with proper values for learning rate and weight decay**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhwvWHnJhb-Q"
      },
      "source": [
        "# TODO: add a decent initial setting and tune from there. The values are intentionally bad.\n",
        "optimizer_config = {\"optimizer_type\": \"adam\", \"lr\": 1e-3, \"weight_decay\": 1e-1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC-V_tTthb-Q"
      },
      "source": [
        "The following cell will take longer than Part 1.3, as now we have more data (and more variability), and the model is slightly more complicated than before as well; however, it should finish within 10~15 minutes anyway, and the default num_epochs is also good enough as a starting point for you to pass this part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBMTF3JZhb-R"
      },
      "source": [
        "simple_model_final = SimpleNetFinal()\n",
        "optimizer = get_optimizer(simple_model_final, optimizer_config)\n",
        "\n",
        "trainer = Trainer(\n",
        "    data_dir=data_path,\n",
        "    model=simple_model_final,\n",
        "    optimizer=optimizer,\n",
        "    model_dir=os.path.join(model_path, \"simple_model_final\"),\n",
        "    train_data_transforms=get_all_transforms(inp_size, [dataset_mean], [dataset_std]),\n",
        "    val_data_transforms=get_fundamental_normalization_transforms(\n",
        "        inp_size, [dataset_mean], [dataset_std]\n",
        "    ),\n",
        "    batch_size=32,\n",
        "    load_from_disk=False,\n",
        "    cuda=is_cuda,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpF7R50whb-R"
      },
      "source": [
        "%%time\n",
        "trainer.run_training_loop(num_epochs=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSvR7Ufjhb-S"
      },
      "source": [
        "Similar to Part 1, now plot out the loss and accuracy history. Also copy the plots onto the report, and answer the questions accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chc9YDS9hb-S"
      },
      "source": [
        "trainer.plot_loss_history()\n",
        "trainer.plot_accuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9wHX7tohb-T"
      },
      "source": [
        "train_accuracy = trainer.train_accuracy_history[-1]\n",
        "validation_accuracy = trainer.validation_accuracy_history[-1]\n",
        "print(\n",
        "    \"Train Accuracy = {}; Validation Accuracy = {}\".format(\n",
        "        train_accuracy, validation_accuracy\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHJVkkG9hb-T"
      },
      "source": [
        "**TODO 13:** Obtain a **55%** validation accuracy to receive full credit for Part 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLUgrlT5hb-T"
      },
      "source": [
        "### Save the model for your SimpleNetFinal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdMA-QIEhb-U"
      },
      "source": [
        "save_trained_model_weights(simple_model_final, out_dir=\"./\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywG8NSYxhb-U"
      },
      "source": [
        "### Part 2.6: Analysis using confusion matrix\n",
        "A confusion matrix is a helpful tool for visualizing the performance of classification algorithms. Each row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class. The confusion matrix counts the number of instances of a given (target, prediction) pair. We are able to use this to understand the classification behaviour.\n",
        "\n",
        "A confusion matrix can also be normalized by dividing each row by the total number of instances of the target class. This is helpful for comparing between large and small datasets, as well as when there is significant class imbalance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xhbUUMDhb-U"
      },
      "source": [
        "**TODO 14:** Do the following to visualize the confusion matrix:\n",
        "\n",
        "1. Implement the code to extract the predictions and targets from a model and a dataset\n",
        "2. Implement the code to generate the confusion matrix, and its normalized form\n",
        "3. Plot the confusion matrix and try to understand how your model is performing, and where it falls short. We'll use this later on for the report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47oQB8Y5hb-V"
      },
      "source": [
        "print(verify(test_generate_confusion_matrix))\n",
        "print(verify(test_generate_confusion_matrix_normalized))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alg_NXDphb-V"
      },
      "source": [
        "%%time\n",
        "targets, predictions, class_labels = generate_confusion_data(\n",
        "    trainer.model, trainer.val_dataset, use_cuda=is_cuda\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiAcQIdWhb-V"
      },
      "source": [
        "confusion_matrix = generate_confusion_matrix(targets, predictions, len(class_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUzxZ1-Qhb-W"
      },
      "source": [
        "plot_confusion_matrix(confusion_matrix, class_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amh1wxlJBpHj"
      },
      "source": [
        "## Part 3: ResNet\n",
        "You can see that after the above adjustment, our model performance increases in terms of testing accuracy. Although the training accuracy drops, now it's closer to the testing values and that's more natural in terms of performance. But we are not satisfied with the final performance yet. Our model, in the end, is still a 2-layer SimpleNet and it might be capable of capturing some features, but could be improved a lot if we go **deeper**. In this part we are going to see the power of a famous model: ResNet18."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttbhBZ7CXUng"
      },
      "source": [
        "inp_size = (224, 224)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGzfNfMivgjb"
      },
      "source": [
        "### Part 3.1 & 3.2: Fine-tuning the ResNet\n",
        "Now let's define a ResNet which can be fit onto our dataset. PyTorch has provided us with pre-trained models like ResNet18, so what you want to do is to load the model first, and then adjust some of the layers such that it fits with our own dataset, instead of outputing scores to 1000 classes from the original ResNet18 model.\n",
        "\n",
        "\n",
        "**TODO 15:** Switch to `my_resnet.py`, and copy the network architecture and weights of all but the last fc layers from the pretrained network.\n",
        "\n",
        "After you have defined the correct architecture of the model, make some tweaks to the existing layers: **freeze** the **convolutional** layers and first 2 **linear** layers so we don't update the weights of them; more details can be found in the instruction webpage.\n",
        "\n",
        "Note that you are allowed to add more layers/unfreeze more layers if you see fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ruw-dg8avgjb"
      },
      "source": [
        "print(\"Testing your ResNet architecture: \", verify(test_my_resnet))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBoLgRrlBpHl"
      },
      "source": [
        "my_resnet = MyResNet18()\n",
        "print(my_resnet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6AYkHAgBpHw"
      },
      "source": [
        "# TODO: add a decent initial setting and tune from there. The values are intentionally bad.\n",
        "optimizer_config = {\"optimizer_type\": \"sgd\", \"lr\": 1e-10, \"weight_decay\": 1e-1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtCIaTMmBpIK"
      },
      "source": [
        "my_resnet = MyResNet18()\n",
        "optimizer = get_optimizer(my_resnet, optimizer_config)\n",
        "\n",
        "trainer = Trainer(\n",
        "    data_dir=data_path,\n",
        "    model=my_resnet,\n",
        "    optimizer=optimizer,\n",
        "    model_dir=os.path.join(model_path, \"resnet18\"),\n",
        "    train_data_transforms=get_all_transforms(inp_size, [dataset_mean], [dataset_std]),\n",
        "    val_data_transforms=get_fundamental_normalization_transforms(\n",
        "        inp_size, [dataset_mean], [dataset_std]\n",
        "    ),\n",
        "    batch_size=32,\n",
        "    load_from_disk=False,\n",
        "    cuda=is_cuda,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYMrCqMgvgjd"
      },
      "source": [
        "The following training cell will take roughly 20 minutes or slightly more using CPU (but possibly under 5 minute using GPU depending on the batch size; the TAs got it within 3 minutes on a GTX1060)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAcncwLPBpIQ"
      },
      "source": [
        "%%time\n",
        "trainer.run_training_loop(num_epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biGF45_bvgje"
      },
      "source": [
        "Like both previous sections, you are required to pass a threshold of **85%** for this part. Copy the plots and values onto the report and answer questions accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cimj95G_BpIU",
        "scrolled": false
      },
      "source": [
        "trainer.plot_loss_history()\n",
        "trainer.plot_accuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMI3CdEuBpIb"
      },
      "source": [
        "train_accuracy = trainer.train_accuracy_history[-1]\n",
        "validation_accuracy = trainer.validation_accuracy_history[-1]\n",
        "print(\n",
        "    \"Train Accuracy = {}; Validation Accuracy = {}\".format(\n",
        "        train_accuracy, validation_accuracy\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_9mBr4fhb-b"
      },
      "source": [
        "**TODO 16**: Obtain a **85%** validation accuracy to receive full credits for Part 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwmEfkaohb-b"
      },
      "source": [
        "### Save Trained MyResnet18 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JttK_45Yhb-b"
      },
      "source": [
        "save_trained_model_weights(my_resnet, out_dir=\"./\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXwDzD5ghb-c"
      },
      "source": [
        "### Part 2.3 Visualize and Analyze Confusion Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "876eAnZZhb-c"
      },
      "source": [
        "**TODO 17:** Visualize and analyze the confusion matrix.\n",
        "\n",
        "You'll need to find an example of an image that is misclassified for the report. Use the confusion matrix and the `get_pred_images_for_target` function to help your analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpoROFbqhb-c"
      },
      "source": [
        "generate_and_plot_confusion_matrix(my_resnet, trainer.val_dataset, use_cuda=is_cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vimh9O8fhb-d"
      },
      "source": [
        "#########################\n",
        "# Use this cell to visualize your images depending on the confusion matrix visualization\n",
        "#########################"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}